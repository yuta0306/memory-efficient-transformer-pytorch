[tool.poetry]
name = "memory-efficient-transformer-pytorch"
version = "0.2.0"
description = "Implementation on PyTorch of Self-attention Does Not Need $O(n^2)$ Memory"
authors = ["yuta0306 <you-2001-3-6@ezweb.ne.jp>"]
license = "MIT"
homepage = "https://github.com/yuta0306/memory-efficient-transformer-pytorch"
repository = "https://github.com/yuta0306/memory-efficient-transformer-pytorch"
readme = "README.md"
packages = [
    { include = "memory_efficient_transformer" },
]

[tool.poetry.dependencies]
python = ">=3.7,<3.11"
torch = ">=1.10.2"
numpy = ">=1.21.6"
tqdm = ">=4.64.0"
transformers = ">=4.20.1"

[tool.poetry.dev-dependencies]
pytest = "^5.2"
black = "^22.1.0"
mypy = "^0.931"
isort = "^5.10.1"
flake8 = "^4.0.1"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
